{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import random\n",
    "import matplotlib.dates as mdates\n",
    "from tensorflow.keras.layers import Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Bidirectional, Attention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense, Attention, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = pd.read_csv(r'Database2.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.sort_values('Date', inplace=True)\n",
    "\n",
    "data['ExchangeRate_Short_MA'] = data['ExchangeRate'].rolling(window=20, min_periods=1).mean()\n",
    "data['ExchangeRate_Long_MA'] = data['ExchangeRate'].rolling(window=80, min_periods=1).mean()\n",
    "\n",
    "data.dropna(inplace=True)"
   ],
   "id": "a1f275976f56cb4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "selected_features = [\n",
    "    'Price', 'STI', 'GoldPrice', 'DXI',\n",
    "    'USD_EUR_ExchangeRate', 'USD_CNY_ExchangeRate',\n",
    "    'ExchangeRate_Short_MA', 'ExchangeRate_Long_MA',\n",
    "    'impact_score'\n",
    "]\n",
    "features = data[selected_features]\n",
    "target = data['ExchangeRate']\n",
    "\n",
    "features.dropna(inplace=True)\n",
    "target.dropna(inplace=True)"
   ],
   "id": "ebc6c296ce1b6b42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scaler_features = MinMaxScaler()\n",
    "features_scaled = scaler_features.fit_transform(features)\n",
    "scaler_target = MinMaxScaler()\n",
    "target_scaled = scaler_target.fit_transform(target.values.reshape(-1, 1))"
   ],
   "id": "12deb647696a99a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(features_scaled, target_scaled.ravel())\n",
    "rf_predictions = rf_model.predict(features_scaled)\n",
    "rf_predictions_reshaped = rf_predictions.reshape(-1, 1)"
   ],
   "id": "fc9edb2c5d7d6589"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "extended_features = np.hstack((features_scaled, rf_predictions_reshaped))\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps), :]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)"
   ],
   "id": "f5d1669830b34a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_predictions(real_vals, lstm_predicted_vals, title=\"Predictions vs. Actual\", num_dates=10):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "\n",
    "    dates = data['Date'].iloc[train_size:train_size + len(real_vals)].map(mdates.date2num)\n",
    "\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "\n",
    "\n",
    "    baseline_predicted_vals = np.roll(real_vals, 3)\n",
    "    baseline_predicted_vals[:3] = real_vals[:3]\n",
    "\n",
    "    plt.plot(dates, real_vals, label='Actual Exchange Rate', color='blue')\n",
    "    plt.plot(dates, lstm_predicted_vals, label='LSTM Predicted Exchange Rate', color='red')\n",
    "    plt.plot(dates, baseline_predicted_vals, label='Baseline Predicted Exchange Rate', color='green')\n",
    "\n",
    "    random_indices = random.sample(range(len(real_vals)), num_dates)\n",
    "    for idx in random_indices:\n",
    "        date = data.iloc[train_size + idx]['Date']\n",
    "        plt.scatter(mdates.date2num(date), real_vals[idx], color='blue')\n",
    "        plt.scatter(mdates.date2num(date), lstm_predicted_vals[idx], color='red')\n",
    "        plt.scatter(mdates.date2num(date), baseline_predicted_vals[idx], color='green')\n",
    "    print(\n",
    "        f\"Date: {date.strftime('%Y-%m-%d')}, Actual: {real_vals[idx]}, LSTM Predicted: {lstm_predicted_vals[idx]}, Baseline Predicted: {baseline_predicted_vals[idx]}\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Exchange Rate')\n",
    "    plt.legend()\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    lstm_mae = np.mean(np.abs(real_vals - lstm_predicted_vals))\n",
    "    baseline_mae = np.mean(np.abs(real_vals - baseline_predicted_vals))\n",
    "\n",
    "    print(f\"Average MAE between LSTM predictions and actual values: {lstm_mae}\")\n",
    "    print(f\"Average MAE between baseline predictions and actual values: {baseline_mae}\")\n",
    "\n",
    "    return lstm_mae, baseline_mae"
   ],
   "id": "9a626a2737c076df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "time_steps_list = [3]\n",
    "performance_metrics = {}\n",
    "\n",
    "for time_steps in time_steps_list:\n",
    "    X, y = create_dataset(features_scaled, target_scaled.ravel(), time_steps=time_steps)\n",
    "\n",
    "    train_size = int(len(X) * 0.96)\n",
    "    X_train, X_val = X[:train_size], X[train_size:]\n",
    "    y_train, y_val = y[:train_size], y[train_size:]\n",
    "\n",
    "    input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "    x = Bidirectional(LSTM(200, return_sequences=True, kernel_regularizer=l2(0.01)))(input_layer)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(200, return_sequences=True, kernel_regularizer=l2(0.01)))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(200, return_sequences=True, kernel_regularizer=l2(0.01)))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    query = x\n",
    "    value = x\n",
    "    attention = Attention(use_scale=True)([query, value])\n",
    "    attention = Dropout(0.05)(attention)\n",
    "    attention = BatchNormalization()(attention)\n",
    "\n",
    "    x = Bidirectional(LSTM(200, kernel_regularizer=l2(0.01)))(attention)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=256,\n",
    "                        validation_data=(X_val, y_val), verbose=1,\n",
    "                        callbacks=[early_stop]\n",
    "                        )\n",
    "\n",
    "    predicted_val = model.predict(X_val)\n",
    "    predicted_val_rescaled = scaler_target.inverse_transform(predicted_val)\n",
    "    real_val_rescaled = scaler_target.inverse_transform(y_val.reshape(-1, 1))\n",
    "\n",
    "    mse_val = mean_squared_error(real_val_rescaled, predicted_val_rescaled)\n",
    "    rmse_val = np.sqrt(mse_val)\n",
    "    mae_val = mean_absolute_error(real_val_rescaled, predicted_val_rescaled)\n",
    "    r2_val = r2_score(real_val_rescaled, predicted_val_rescaled)\n",
    "\n",
    "    performance_metrics[time_steps] = {\n",
    "        'MSE': mse_val,\n",
    "        'RMSE': rmse_val,\n",
    "        'MAE': mae_val,\n",
    "        'R^2': r2_val\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    plot_predictions(real_val_rescaled, predicted_val_rescaled,\n",
    "                     title=f\"LSTM Model Predictions for time_steps = {time_steps}\")\n"
   ],
   "id": "5c7f9d98b5ccd5b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for time_steps, metrics in performance_metrics.items():\n",
    "    print(f\"Results for time_steps = {time_steps}:\")\n",
    "    print(f\"MSE: {metrics['MSE']}, RMSE: {metrics['RMSE']}, MAE: {metrics['MAE']}, R^2: {metrics['R^2']}\\n\")"
   ],
   "id": "25f55645f5472f98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, BatchNormalization, Attention\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import random\n",
    "import logging"
   ],
   "id": "558ece5e11e9107a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def initialize_particles(num_particles, lower_bound, upper_bound):\n",
    "    particles = np.random.uniform(low=-0.008, high=0.008, size=num_particles)\n",
    "    weights = np.ones(num_particles) / num_particles\n",
    "    return particles, weights"
   ],
   "id": "de11d1ca1a4ba11f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict_particles(particles, lstm_predict_function, features):\n",
    "    predictions = lstm_predict_function(features)\n",
    "    predicted_particles = predictions.flatten() + np.random.normal(0, 0.001, size=particles.shape)\n",
    "    return predicted_particles"
   ],
   "id": "d9e3b1453f44f2f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def update_particle_weights(particles, weights, actual_value, beta=0.1):\n",
    "    particle_differences = np.abs(particles - actual_value)\n",
    "    logging.debug(f\"Before update: max weight={np.max(weights)}, min weight={np.min(weights)}\")\n",
    "    weights *= np.exp(-beta * particle_differences)\n",
    "    weights += 1.e-30000\n",
    "    weights /= np.sum(weights)\n",
    "    logging.debug(f\"After update: max weight={np.max(weights)}, min weight={np.min(weights)}\")\n",
    "    return weights"
   ],
   "id": "a1a1dc7d6b198c3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def stratified_resample(particles, weights):\n",
    "    N = len(particles)\n",
    "    positions = (np.random.rand(N) + range(N)) / N\n",
    "    indexes = np.zeros(N, 'i')\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    i, j = 0, 0\n",
    "    while i < N:\n",
    "        if positions[i] < cumulative_sum[j]:\n",
    "            indexes[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return particles[indexes], np.ones_like(weights) / N"
   ],
   "id": "d5a02e084bd2eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps), :]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps - 1])\n",
    "    return np.array(Xs), np.array(ys)"
   ],
   "id": "c82d5c54af88c135"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_lstm_model(X_train, y_train, X_val, y_val, time_steps):\n",
    "    input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    x = LSTM(200, return_sequences=True, kernel_regularizer=l2(0.01))(input_layer)\n",
    "    x = Dropout(0.05)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    attention = Attention(use_scale=True)([x, x])\n",
    "    attention = Dropout(0.05)(attention)\n",
    "    attention = BatchNormalization()(attention)\n",
    "    x = LSTM(200, kernel_regularizer=l2(0.01))(attention)\n",
    "    x = Dropout(0.05)(x)\n",
    "    output = Dense(1)(x)\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val),\n",
    "                        callbacks=[early_stop])\n",
    "    return model"
   ],
   "id": "4d0b52da417ce0ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def lstm_predict_function(model, scaler, features):\n",
    "    features = np.expand_dims(features, axis=0)\n",
    "    prediction = model.predict(features)\n",
    "    return scaler.inverse_transform(prediction).flatten()"
   ],
   "id": "58f7b0b5478cd075"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_model(model, X_test, y_test, scaler):\n",
    "    predictions = model.predict(X_test)\n",
    "    predicted_vals = scaler.inverse_transform(predictions)\n",
    "\n",
    "    actual_vals = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(actual_vals, label='Actual Values')\n",
    "    plt.plot(predicted_vals, label='Predicted Values', alpha=0.7)\n",
    "    plt.title('LSTM Model Evaluation')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    mse = mean_squared_error(actual_vals, predicted_vals)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual_vals, predicted_vals)\n",
    "    r2_val = r2_score(actual_vals, predicted_vals)\n",
    "    print(f\"MSE: {mse}, RMSE: {rmse}, MAE: {mae}, R^2:{r2_val}\")"
   ],
   "id": "9e2412ec332ec3bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_particle_trajectories(data, real_vals, particle_trajectories, train_size):\n",
    "    with plt.style.context('ggplot'):\n",
    "        plt.figure(figsize=(14, 7))\n",
    "    valid_indices = ~np.isnan(real_vals).flatten()\n",
    "    dates = data.iloc[train_size:train_size + len(real_vals)]['Date'][valid_indices]\n",
    "    real_vals = real_vals[valid_indices]\n",
    "\n",
    "    dates_mapped = dates.map(mdates.date2num)\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "\n",
    "    plt.plot(dates_mapped, real_vals, label='Actual Log Returns', color='blue', alpha=0.75)\n",
    "\n",
    "    for particles_at_time_t in particle_trajectories.T:\n",
    "        plt.scatter(dates_mapped, particles_at_time_t, color='red', alpha=0.05)\n",
    "\n",
    "    plt.title(\"Particle Filter Predictions over Time\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Log Returns')\n",
    "    plt.legend()\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.show()"
   ],
   "id": "15a22094a57edc77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main():\n",
    "    data = pd.read_csv(r'Database2.csv')\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.sort_values('Date', inplace=True)\n",
    "    data['Log_Returns'] = np.log(data['ExchangeRate'] / data['ExchangeRate'].shift(1))\n",
    "    data['Price_Log_Return'] = np.log(data['Price'] / data['Price'].shift(1))\n",
    "    data['STI_Log_Return'] = np.log(data['STI'] / data['STI'].shift(1))\n",
    "    data['GoldPrice_Log_Return'] = np.log(data['GoldPrice'] / data['GoldPrice'].shift(1))\n",
    "    data['USD_EUR_ExchangeRate_Log_Return'] = np.log(\n",
    "    data['USD_EUR_ExchangeRate'] / data['USD_EUR_ExchangeRate'].shift(1))\n",
    "    data['USD_CNY_ExchangeRate_Log_Return'] = np.log(\n",
    "    data['USD_CNY_ExchangeRate'] / data['USD_CNY_ExchangeRate'].shift(1))\n",
    "    data['impact_score_Log_Return'] = np.log(data['impact_score'] / data['impact_score'].shift(1))\n",
    "    data['ExchangeRate_Short_MA'] = data['ExchangeRate'].rolling(window=20, min_periods=1).mean()\n",
    "    data['ExchangeRate_Long_MA'] = data['ExchangeRate'].rolling(window=80, min_periods=1).mean()\n",
    "    data['ExchangeRate_Long_MA_Log_Return'] = np.log(\n",
    "    data['ExchangeRate_Long_MA'] / data['ExchangeRate_Long_MA'].shift(1))\n",
    "    data['ExchangeRate_Short_MA_Log_Return'] = np.log(\n",
    "    data['ExchangeRate_Short_MA'] / data['ExchangeRate_Short_MA'].shift(1))\n",
    "    selected_features = [\n",
    "        'Price_Log_Return', 'STI_Log_Return', 'GoldPrice_Log_Return',\n",
    "        'USD_EUR_ExchangeRate_Log_Return', 'USD_CNY_ExchangeRate_Log_Return',\n",
    "        'ExchangeRate_Long_MA_Log_Return', 'ExchangeRate_Short_MA_Log_Return',\n",
    "        'impact_score_Log_Return'\n",
    "        ]\n",
    "    features = data[selected_features]\n",
    "    target = data['Log_Returns']\n",
    "    features = data[selected_features].dropna()\n",
    "    target = data['Log_Returns'].dropna()\n",
    "    scaler_features = MinMaxScaler()\n",
    "    features_scaled = scaler_features.fit_transform(features)\n",
    "    scaler_target = MinMaxScaler()\n",
    "    target_scaled = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "    X, y = create_dataset(features_scaled, target_scaled.ravel(), time_steps=3)\n",
    "    train_size = int(len(X) * 0.96)\n",
    "    X_train, X_val = X[:train_size], X[train_size:]\n",
    "    y_train, y_val = y[:train_size], y[train_size:]\n",
    "    model = train_lstm_model(X_train, y_train, X_val, y_val, 3)\n",
    "    predicted_val = model.predict(X_val)\n",
    "    predicted_val_rescaled = scaler_target.inverse_transform(predicted_val)\n",
    "    real_val_rescaled = scaler_target.inverse_transform(y_val.reshape(-1, 1))\n",
    "\n",
    "    num_particles = 1000\n",
    "    particles, weights = initialize_particles(num_particles, lower_bound=-0.05, upper_bound=0.05)\n",
    "    particle_trajectories = np.zeros((len(X_val), num_particles))\n",
    "\n",
    "    for i in range(len(X_val)):\n",
    "        current_features = X_val[i]\n",
    "        actual_value = y_val[i]\n",
    "\n",
    "        particles = predict_particles(particles, lambda features: lstm_predict_function(model, scaler_target, features),\n",
    "                                      current_features)\n",
    "        weights = update_particle_weights(particles, weights, actual_value)\n",
    "        particles, weights = stratified_resample(particles, weights)\n",
    "        particle_trajectories[i] = particles\n",
    "\n",
    "    plot_particle_trajectories(data, real_val_rescaled, particle_trajectories, train_size)\n",
    "    evaluate_model(model, X_val, y_val, scaler_target)"
   ],
   "id": "fc999ed4e235ade6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "2311234e510878f5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
